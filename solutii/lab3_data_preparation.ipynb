{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laborator 3: Pregătirea și Curățarea Datelor\n",
    "\n",
    "## Obiective\n",
    "- Încărcarea și explorarea dataset-ului NSL-KDD\n",
    "- Analiza exploratorie a datelor (EDA)\n",
    "- Tratarea valorilor lipsă și a outlier-ilor\n",
    "- Encoding pentru variabile categorice\n",
    "- Normalizare/Standardizare\n",
    "- Împărțirea în train/test sets\n",
    "\n",
    "## Dataset: NSL-KDD\n",
    "NSL-KDD este un dataset îmbunătățit pentru detectarea intruziunilor în rețea, derivat din KDD Cup 99.\n",
    "\n",
    "**Clase de atacuri:**\n",
    "- `normal` - trafic legitim\n",
    "- `DoS` - Denial of Service\n",
    "- `Probe` - Scanare și recunoaștere\n",
    "- `R2L` - Remote to Local\n",
    "- `U2R` - User to Root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup și Import Biblioteci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalare dependențe (rulați doar în Google Colab)\n",
    "# !pip install pandas numpy scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import biblioteci\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Setări afișare\n",
    "pd.set_option('display.max_columns', 50)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"Biblioteci încărcate cu succes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Încărcarea Datelor\n",
    "\n",
    "Dataset-ul NSL-KDD poate fi descărcat de pe Kaggle sau de pe site-ul UNB.\n",
    "\n",
    "**Opțiuni de încărcare:**\n",
    "1. Upload manual în Colab\n",
    "2. Descărcare directă din URL\n",
    "3. Conectare la Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definim numele coloanelor pentru NSL-KDD\n",
    "# (dataset-ul original nu are header)\n",
    "\n",
    "column_names = [\n",
    "    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes',\n",
    "    'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in',\n",
    "    'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations',\n",
    "    'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login',\n",
    "    'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate',\n",
    "    'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate',\n",
    "    'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count',\n",
    "    'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',\n",
    "    'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate',\n",
    "    'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'label', 'difficulty'\n",
    "]\n",
    "\n",
    "print(f\"Număr total de coloane: {len(column_names)}\")\n",
    "print(f\"Coloane categorice: protocol_type, service, flag, label\")\n",
    "print(f\"Coloane numerice: restul de {len(column_names) - 4} coloane\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPȚIUNEA 1: Upload manual în Colab\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "\n",
    "# OPȚIUNEA 2: Încărcare din fișier local sau URL\n",
    "# Pentru acest laborator, vom folosi o versiune sintetică a dataset-ului\n",
    "# În practică, descărcați de pe Kaggle: https://www.kaggle.com/datasets/hassan06/nslkdd\n",
    "\n",
    "# Creăm un dataset sintetic pentru demonstrație\n",
    "# (Înlocuiți cu calea reală către fișierul descărcat)\n",
    "\n",
    "def create_sample_dataset(n_samples=10000):\n",
    "    \"\"\"Creează un dataset sintetic similar cu NSL-KDD pentru demonstrație.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    data = {\n",
    "        'duration': np.random.exponential(100, n_samples),\n",
    "        'protocol_type': np.random.choice(['tcp', 'udp', 'icmp'], n_samples, p=[0.7, 0.2, 0.1]),\n",
    "        'service': np.random.choice(['http', 'ftp', 'smtp', 'ssh', 'dns', 'other'], n_samples),\n",
    "        'flag': np.random.choice(['SF', 'S0', 'REJ', 'RSTR', 'SH'], n_samples),\n",
    "        'src_bytes': np.random.exponential(1000, n_samples),\n",
    "        'dst_bytes': np.random.exponential(2000, n_samples),\n",
    "        'land': np.random.choice([0, 1], n_samples, p=[0.99, 0.01]),\n",
    "        'wrong_fragment': np.random.choice([0, 1, 2, 3], n_samples, p=[0.95, 0.03, 0.01, 0.01]),\n",
    "        'urgent': np.random.choice([0, 1], n_samples, p=[0.999, 0.001]),\n",
    "        'hot': np.random.poisson(0.5, n_samples),\n",
    "        'num_failed_logins': np.random.choice([0, 1, 2, 3], n_samples, p=[0.9, 0.07, 0.02, 0.01]),\n",
    "        'logged_in': np.random.choice([0, 1], n_samples, p=[0.3, 0.7]),\n",
    "        'num_compromised': np.random.poisson(0.1, n_samples),\n",
    "        'root_shell': np.random.choice([0, 1], n_samples, p=[0.99, 0.01]),\n",
    "        'su_attempted': np.random.choice([0, 1], n_samples, p=[0.995, 0.005]),\n",
    "        'num_root': np.random.poisson(0.1, n_samples),\n",
    "        'num_file_creations': np.random.poisson(0.2, n_samples),\n",
    "        'num_shells': np.random.choice([0, 1], n_samples, p=[0.99, 0.01]),\n",
    "        'num_access_files': np.random.poisson(0.1, n_samples),\n",
    "        'num_outbound_cmds': np.zeros(n_samples),\n",
    "        'is_host_login': np.random.choice([0, 1], n_samples, p=[0.999, 0.001]),\n",
    "        'is_guest_login': np.random.choice([0, 1], n_samples, p=[0.99, 0.01]),\n",
    "        'count': np.random.poisson(50, n_samples),\n",
    "        'srv_count': np.random.poisson(30, n_samples),\n",
    "        'serror_rate': np.random.beta(0.5, 5, n_samples),\n",
    "        'srv_serror_rate': np.random.beta(0.5, 5, n_samples),\n",
    "        'rerror_rate': np.random.beta(0.5, 10, n_samples),\n",
    "        'srv_rerror_rate': np.random.beta(0.5, 10, n_samples),\n",
    "        'same_srv_rate': np.random.beta(5, 1, n_samples),\n",
    "        'diff_srv_rate': np.random.beta(1, 5, n_samples),\n",
    "        'srv_diff_host_rate': np.random.beta(1, 5, n_samples),\n",
    "        'dst_host_count': np.random.poisson(100, n_samples),\n",
    "        'dst_host_srv_count': np.random.poisson(50, n_samples),\n",
    "        'dst_host_same_srv_rate': np.random.beta(5, 1, n_samples),\n",
    "        'dst_host_diff_srv_rate': np.random.beta(1, 5, n_samples),\n",
    "        'dst_host_same_src_port_rate': np.random.beta(2, 3, n_samples),\n",
    "        'dst_host_srv_diff_host_rate': np.random.beta(1, 5, n_samples),\n",
    "        'dst_host_serror_rate': np.random.beta(0.5, 5, n_samples),\n",
    "        'dst_host_srv_serror_rate': np.random.beta(0.5, 5, n_samples),\n",
    "        'dst_host_rerror_rate': np.random.beta(0.5, 10, n_samples),\n",
    "        'dst_host_srv_rerror_rate': np.random.beta(0.5, 10, n_samples),\n",
    "        'label': np.random.choice(['normal', 'neptune', 'satan', 'ipsweep', 'portsweep', \n",
    "                                   'smurf', 'nmap', 'back', 'teardrop', 'warezclient'],\n",
    "                                  n_samples, p=[0.5, 0.15, 0.05, 0.05, 0.05, \n",
    "                                               0.05, 0.05, 0.03, 0.04, 0.03]),\n",
    "        'difficulty': np.random.randint(1, 22, n_samples)\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Creăm dataset-ul\n",
    "df = create_sample_dataset(10000)\n",
    "\n",
    "print(f\"Dataset încărcat: {df.shape[0]} rânduri x {df.shape[1]} coloane\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primele 5 rânduri\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informații despre dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Înainte de a preprocesa datele, trebuie să înțelegem:\n",
    "- Distribuția claselor (echilibru/dezechilibru)\n",
    "- Tipurile de date\n",
    "- Valori lipsă\n",
    "- Statistici descriptive\n",
    "- Corelații între features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistici descriptive pentru coloanele numerice\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificare valori lipsă\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Valori lipsă per coloană:\")\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"\\nNu există valori lipsă în dataset! ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribuția claselor (label)\n",
    "print(\"Distribuția claselor:\")\n",
    "label_counts = df['label'].value_counts()\n",
    "print(label_counts)\n",
    "\n",
    "# Vizualizare\n",
    "plt.figure(figsize=(12, 5))\n",
    "label_counts.plot(kind='bar', color='steelblue', edgecolor='black')\n",
    "plt.title('Distribuția Claselor în Dataset', fontsize=14)\n",
    "plt.xlabel('Tip Trafic/Atac')\n",
    "plt.ylabel('Număr Instanțe')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapare atacuri la categorii principale\n",
    "attack_mapping = {\n",
    "    'normal': 'normal',\n",
    "    'neptune': 'DoS', 'smurf': 'DoS', 'back': 'DoS', 'teardrop': 'DoS', \n",
    "    'pod': 'DoS', 'land': 'DoS',\n",
    "    'satan': 'Probe', 'ipsweep': 'Probe', 'nmap': 'Probe', 'portsweep': 'Probe',\n",
    "    'warezclient': 'R2L', 'guess_passwd': 'R2L', 'warezmaster': 'R2L', \n",
    "    'imap': 'R2L', 'ftp_write': 'R2L',\n",
    "    'buffer_overflow': 'U2R', 'rootkit': 'U2R', 'loadmodule': 'U2R', 'perl': 'U2R'\n",
    "}\n",
    "\n",
    "# Creăm o nouă coloană pentru categoria de atac\n",
    "df['attack_category'] = df['label'].map(attack_mapping)\n",
    "\n",
    "# Unele etichete pot să nu fie în mapping, le marcăm ca 'other'\n",
    "df['attack_category'] = df['attack_category'].fillna('other')\n",
    "\n",
    "print(\"\\nDistribuția categoriilor de atacuri:\")\n",
    "print(df['attack_category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vizualizare categorii de atacuri\n",
    "plt.figure(figsize=(10, 6))\n",
    "category_counts = df['attack_category'].value_counts()\n",
    "colors = ['#2ecc71', '#e74c3c', '#3498db', '#9b59b6', '#f39c12']\n",
    "plt.pie(category_counts.values, labels=category_counts.index, autopct='%1.1f%%',\n",
    "        colors=colors[:len(category_counts)], explode=[0.05]*len(category_counts))\n",
    "plt.title('Distribuția Categoriilor de Atacuri', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribuția protocoalelor\n",
    "plt.figure(figsize=(8, 5))\n",
    "df['protocol_type'].value_counts().plot(kind='bar', color=['#3498db', '#e74c3c', '#2ecc71'])\n",
    "plt.title('Distribuția Protocoalelor', fontsize=14)\n",
    "plt.xlabel('Protocol')\n",
    "plt.ylabel('Număr Instanțe')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corelații între features numerice (selectăm un subset pentru vizibilitate)\n",
    "numeric_cols = ['duration', 'src_bytes', 'dst_bytes', 'count', 'srv_count', \n",
    "                'serror_rate', 'same_srv_rate', 'dst_host_count']\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlation_matrix = df[numeric_cols].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "            fmt='.2f', linewidths=0.5)\n",
    "plt.title('Matricea de Corelație (Subset Features)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocesarea Datelor\n",
    "\n",
    "### 4.1 Separarea Features și Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creăm target binar: normal vs atac\n",
    "df['is_attack'] = (df['label'] != 'normal').astype(int)\n",
    "\n",
    "print(\"Distribuție target binar:\")\n",
    "print(df['is_attack'].value_counts())\n",
    "print(f\"\\nProcentaj atacuri: {df['is_attack'].mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separăm features de target\n",
    "# Eliminăm coloanele care nu sunt features\n",
    "columns_to_drop = ['label', 'difficulty', 'attack_category', 'is_attack']\n",
    "\n",
    "X = df.drop(columns=columns_to_drop)\n",
    "y = df['is_attack']  # sau df['attack_category'] pentru clasificare multi-clasă\n",
    "\n",
    "print(f\"Shape features (X): {X.shape}\")\n",
    "print(f\"Shape target (y): {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Encoding - Convertirea Variabilelor Categorice\n",
    "\n",
    "Avem 3 coloane categorice:\n",
    "- `protocol_type`: tcp, udp, icmp\n",
    "- `service`: http, ftp, smtp, etc.\n",
    "- `flag`: SF, S0, REJ, etc.\n",
    "\n",
    "**Opțiuni de encoding:**\n",
    "1. **Label Encoding** - transformă în numere (0, 1, 2, ...)\n",
    "2. **One-Hot Encoding** - creează coloane binare pentru fiecare valoare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificăm coloanele categorice\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"Coloane categorice: {categorical_cols}\")\n",
    "\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col}: {X[col].nunique()} valori unice\")\n",
    "    print(X[col].value_counts().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metoda 1: Label Encoding\n",
    "# Bun pentru arbori de decizie, dar poate introduce ordine falsă\n",
    "\n",
    "X_label_encoded = X.copy()\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X_label_encoded[col] = le.fit_transform(X[col])\n",
    "    label_encoders[col] = le\n",
    "    print(f\"{col}: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
    "\n",
    "print(\"\\nPrimele 5 rânduri după Label Encoding:\")\n",
    "X_label_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metoda 2: One-Hot Encoding\n",
    "# Nu introduce ordine falsă, dar crește numărul de coloane\n",
    "\n",
    "X_onehot = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "print(f\"Shape înainte de One-Hot: {X.shape}\")\n",
    "print(f\"Shape după One-Hot: {X_onehot.shape}\")\n",
    "print(f\"\\nColoane noi create: {X_onehot.shape[1] - X.shape[1] + len(categorical_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vom folosi Label Encoding pentru simplitate\n",
    "# (În practică, One-Hot e mai bun pentru rețele neurale și regresie logistică)\n",
    "\n",
    "X_processed = X_label_encoded.copy()\n",
    "print(f\"\\nShape final features: {X_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Normalizare / Standardizare\n",
    "\n",
    "De ce e important?\n",
    "- Features cu valori mari domină calculele\n",
    "- Algoritmii bazați pe distanță (KNN) sunt afectați\n",
    "- Gradienții în rețele neurale converg mai bine\n",
    "\n",
    "**Opțiuni:**\n",
    "1. **StandardScaler**: (x - mean) / std → valori centrate pe 0\n",
    "2. **MinMaxScaler**: (x - min) / (max - min) → valori în [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificăm range-ul valorilor înainte de scalare\n",
    "print(\"Range valori înainte de scalare:\")\n",
    "print(X_processed.describe().loc[['min', 'max', 'mean', 'std']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScaler - pentru majoritatea algoritmilor ML\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_processed)\n",
    "\n",
    "# Convertim înapoi la DataFrame pentru vizualizare\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X_processed.columns)\n",
    "\n",
    "print(\"Range valori după StandardScaler:\")\n",
    "print(X_scaled_df.describe().loc[['min', 'max', 'mean', 'std']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternativ: MinMaxScaler - pentru rețele neurale\n",
    "minmax_scaler = MinMaxScaler()\n",
    "X_minmax = minmax_scaler.fit_transform(X_processed)\n",
    "X_minmax_df = pd.DataFrame(X_minmax, columns=X_processed.columns)\n",
    "\n",
    "print(\"Range valori după MinMaxScaler:\")\n",
    "print(X_minmax_df.describe().loc[['min', 'max', 'mean', 'std']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vizualizare comparativă a distribuției unui feature înainte/după scalare\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "feature = 'src_bytes'\n",
    "\n",
    "axes[0].hist(X_processed[feature], bins=50, color='steelblue', edgecolor='black')\n",
    "axes[0].set_title(f'{feature} - Original')\n",
    "axes[0].set_xlabel('Valoare')\n",
    "\n",
    "axes[1].hist(X_scaled_df[feature], bins=50, color='green', edgecolor='black')\n",
    "axes[1].set_title(f'{feature} - StandardScaler')\n",
    "axes[1].set_xlabel('Valoare')\n",
    "\n",
    "axes[2].hist(X_minmax_df[feature], bins=50, color='orange', edgecolor='black')\n",
    "axes[2].set_title(f'{feature} - MinMaxScaler')\n",
    "axes[2].set_xlabel('Valoare')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Train/Test Split\n",
    "\n",
    "Împărțim datele în:\n",
    "- **Train set** (80%): pentru antrenarea modelului\n",
    "- **Test set** (20%): pentru evaluarea finală\n",
    "\n",
    "**Important:** Folosim `stratify` pentru a păstra proporția claselor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Împărțire stratificată\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled,  # Folosim datele scalate\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y  # Păstrează proporția claselor\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificăm că proporțiile sunt păstrate\n",
    "print(\"\\nDistribuție clase în setul original:\")\n",
    "print(y.value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nDistribuție clase în train set:\")\n",
    "print(pd.Series(y_train).value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nDistribuție clase în test set:\")\n",
    "print(pd.Series(y_test).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Salvarea Datelor Preprocesate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvăm datele preprocesate pentru laboratoarele următoare\n",
    "import pickle\n",
    "\n",
    "# Salvare în format numpy\n",
    "np.save('X_train.npy', X_train)\n",
    "np.save('X_test.npy', X_test)\n",
    "np.save('y_train.npy', y_train)\n",
    "np.save('y_test.npy', y_test)\n",
    "\n",
    "# Salvare scaler pentru utilizare ulterioară\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Salvare label encoders\n",
    "with open('label_encoders.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoders, f)\n",
    "\n",
    "print(\"Date salvate cu succes!\")\n",
    "print(\"Fișiere create:\")\n",
    "print(\"  - X_train.npy, X_test.npy\")\n",
    "print(\"  - y_train.npy, y_test.npy\")\n",
    "print(\"  - scaler.pkl, label_encoders.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvare și în format CSV pentru backup\n",
    "train_df = pd.DataFrame(X_train, columns=X_processed.columns)\n",
    "train_df['target'] = y_train.values\n",
    "train_df.to_csv('train_processed.csv', index=False)\n",
    "\n",
    "test_df = pd.DataFrame(X_test, columns=X_processed.columns)\n",
    "test_df['target'] = y_test.values\n",
    "test_df.to_csv('test_processed.csv', index=False)\n",
    "\n",
    "print(\"Fișiere CSV create: train_processed.csv, test_processed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Rezumat\n",
    "\n",
    "În acest laborator am realizat:\n",
    "\n",
    "1. **Încărcarea datelor** - Dataset NSL-KDD cu 41 features\n",
    "2. **EDA** - Analiza distribuției claselor, protocoalelor, corelațiilor\n",
    "3. **Encoding** - Label Encoding pentru variabile categorice\n",
    "4. **Scalare** - StandardScaler pentru normalizarea valorilor\n",
    "5. **Train/Test Split** - 80/20 cu stratificare\n",
    "\n",
    "### Features finale:\n",
    "- 38 features numerice (după encoding)\n",
    "- Toate valorile scalate cu mean=0, std=1\n",
    "- Target binar: 0=normal, 1=atac\n",
    "\n",
    "### Următorul pas:\n",
    "**Laborator 4** - Vom antrena modele de Machine Learning clasic (Decision Tree, Random Forest, KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sumar final\n",
    "print(\"=\" * 50)\n",
    "print(\"SUMAR PREPROCESARE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nDataset original: {df.shape[0]} samples x {df.shape[1]} features\")\n",
    "print(f\"Features după encoding: {X_processed.shape[1]}\")\n",
    "print(f\"\\nTrain set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"\\nClase: 0=normal ({(y==0).sum()}), 1=atac ({(y==1).sum()})\")\n",
    "print(f\"Raport atacuri: {y.mean()*100:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
